{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import IPython\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import PIL\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw as PILImageDraw\n",
    "import tensorflow as tf\n",
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import math\n",
    "from typing import Any, Callable,Dict, List, Optional, Sequence, Tuple, Union\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil \n",
    "import os \n",
    "import base64\n",
    "import torch\n",
    "import albumentations as A\n",
    "from functools import wraps\n",
    "\n",
    "# ### Dependencies\n",
    "# \n",
    "# #### All dependencies are in requirements.txt\n",
    "# \n",
    "# ( Use this command to generate the requirement.txt \n",
    "# **conda list -e > requirements.txt** )\n",
    "# \n",
    "# Install albumentations using the following commands\n",
    "# * pip install -U albumentations\n",
    "# * pip install -U git+https://github.com/albumentations-team/albumentations\n",
    "# \n",
    "# Why albumentations?\n",
    "# https://docs.google.com/spreadsheets/d/1rmaGngJXj3X0_ugVLWVW7h4lvayWiIJO_o2dfRNQ380/edit?usp=sharing\n",
    "\n",
    "# ### Transform Functions \n",
    "# *    Blur\n",
    "# *    CLAHE\n",
    "# *    ChannelDropout\n",
    "# *    ChannelShuffle\n",
    "# *    ColorJitter\n",
    "# *    Downscale\n",
    "# *    Emboss\n",
    "# *    Equalize\n",
    "# *    FDA\n",
    "# *    FancyPCA\n",
    "# *    FromFloat\n",
    "# *    GaussNoise\n",
    "# *    GaussianBlur\n",
    "# *    GlassBlur\n",
    "# *    HistogramMatching\n",
    "# *    HueSaturationValue\n",
    "# *    ISONoise\n",
    "# *    ImageCompression\n",
    "# *    InvertImg\n",
    "# *    MedianBlur\n",
    "# *    MotionBlur\n",
    "# *    MultiplicativeNoise\n",
    "# *    Normalize\n",
    "# *    PixelDistributionAdaptation\n",
    "# *    Posterize\n",
    "# *    RGBShift\n",
    "# *    Sharpen\n",
    "# *    Solarize\n",
    "# *    Superpixels\n",
    "# *    ToFloat\n",
    "# *    ToGray\n",
    "# *    ToSepia\n",
    "# *    VerticalFlip\n",
    "# *    HorizontalFlip\n",
    "# *    Flip (Random_Flip Code Commented)\n",
    "# *    Transpose\n",
    "# *    OpticalDistortion\n",
    "# *    GridDistortion\n",
    "# *    PadIfNeeded\n",
    "# *    JpegCompression\n",
    "# *    Cutout\n",
    "# *    CoarseDropout\n",
    "# *    Lambda\n",
    "# *    MaskDropout\n",
    "# *    GridDropout\n",
    "# *    TemplateTransform\n",
    "# *    RingingOvershoot\n",
    "# *    UnsharpMask\n",
    "# \n",
    "# ### =========================================\n",
    "# \n",
    "# #### Random Custom Functions\n",
    "#  \n",
    "# *    Random_crop\n",
    "# *    Random_resize\n",
    "# *    Random_scale\n",
    "# *    Random_rotate\n",
    "# *    Random_shift_scale_rotate\n",
    "# \n",
    "# ### =========================================\n",
    "# \n",
    "# #### To be added\n",
    "# \n",
    "# Random Custom Functions\n",
    "# \n",
    "# *    RandomBrightnessContrast\n",
    "# *    RandomFog\n",
    "# *    RandomGamma\n",
    "# *    RandomRain\n",
    "# *    RandomShadow\n",
    "# *    RandomSnow\n",
    "# *    RandomSunFlare\n",
    "# *    RandomToneCurve\n",
    "# *    RandomBrightness\n",
    "# *    RandomContrast\n",
    "# *    RandomGridShuffle\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "# #### Blur\n",
    "\n",
    "\n",
    "def Blur(image,blur_limit=7, always_apply=False, p=1.0):\n",
    "\n",
    "    \"\"\"Blur the input image using a random-sized kernel.\n",
    "    Args:\n",
    "        blur_limit (int, (int, int)): maximum kernel size for blurring the input image.\n",
    "            Should be in range [3, inf). Default: (3, 7).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Blur(blur_limit, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "# #### CLAHE ( Contrast Limited Adaptive Histogram Equalization )\n",
    "\n",
    "\n",
    "def CLAHE(image,clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=1.0):\n",
    "    \"\"\"Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n",
    "    Args:\n",
    "        clip_limit (float or (float, float)): upper threshold value for contrast limiting.\n",
    "            If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n",
    "        tile_grid_size ((int, int)): size of grid for histogram equalization. Default: (8, 8).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.CLAHE(clip_limit, tile_grid_size, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ChannelDropout\n",
    "\n",
    "\n",
    "def ChannelDropout(image,channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1.0):\n",
    "    \"\"\"Randomly Drop Channels in the input Image.\n",
    "    Args:\n",
    "        channel_drop_range (int, int): range from which we choose the number of channels to drop.\n",
    "        fill_value (int, float): pixel value for the dropped channel.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, uint16, unit32, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ChannelDropout (channel_drop_range, fill_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ChannelShuffle\n",
    "\n",
    "\n",
    "def ChannelShuffle(image,p=1.0):\n",
    "    \"\"\"Randomly rearrange channels of the input RGB image.\n",
    "    Args:\n",
    "    p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "    image\n",
    "    Image types:\n",
    "    uint8, float32\n",
    "    \"\"\"\n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ChannelShuffle(p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ColorJitter\n",
    "\n",
    "\n",
    "def ColorJitter (image,brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=1.0):\n",
    "    \"\"\"Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision,\n",
    "    this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in\n",
    "    Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8\n",
    "    overflow, but we use value saturation.\n",
    "    Args:\n",
    "    brightness (float or tuple of float (min, max)): How much to jitter brightness.\n",
    "        brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n",
    "        or the given [min, max]. Should be non negative numbers.\n",
    "    contrast (float or tuple of float (min, max)): How much to jitter contrast.\n",
    "        contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n",
    "        or the given [min, max]. Should be non negative numbers.\n",
    "    saturation (float or tuple of float (min, max)): How much to jitter saturation.\n",
    "        saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n",
    "        or the given [min, max]. Should be non negative numbers.\n",
    "    hue (float or tuple of float (min, max)): How much to jitter hue.\n",
    "        hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n",
    "        Should have 0 <= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.ColorJitter (brightness, contrast, saturation, hue, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Downscale\n",
    "\n",
    "\n",
    "def Downscale (image,scale_min=0.25, scale_max=0.25, interpolation=0, always_apply=False, p=1.0):\n",
    "    \"\"\"Decreases image quality by downscaling and upscaling back.\n",
    "    Args:\n",
    "        scale_min (float): lower bound on the image scale. Should be < 1.\n",
    "        scale_max (float):  lower bound on the image scale. Should be .\n",
    "        interpolation: cv2 interpolation method. cv2.INTER_NEAREST by default\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Downscale(scale_min, scale_max, interpolation, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Emboss\n",
    "\n",
    "\n",
    "def Emboss (image,alpha=(0.2, 0.5), strength=(0.2, 0.7), always_apply=False, p=1.0):\n",
    "    \"\"\"Emboss the input image and overlays the result with the original image.\n",
    "    Args:\n",
    "        alpha ((float, float)): range to choose the visibility of the embossed image. At 0, only the original image is\n",
    "            visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5).\n",
    "        strength ((float, float)): strength range of the embossing. Default: (0.2, 0.7).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Emboss (alpha, strength, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Equalize\n",
    "\n",
    "\n",
    "def Equalize (image,mode='cv', by_channels=True, mask=None, mask_params=None, always_apply=False, p=1.0):\n",
    "    \"\"\"Equalize the image histogram.\n",
    "    Args:\n",
    "        mode (str): {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n",
    "        by_channels (bool): If True, use equalization by channels separately,\n",
    "            else convert image to YCbCr representation and use equalization by `Y` channel.\n",
    "        mask (np.ndarray, callable): If given, only the pixels selected by\n",
    "            the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable.\n",
    "            Function signature must include `image` argument.\n",
    "        mask_params (list of str): Params for mask function.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Equalize (mode, by_channels, mask, mask_params, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### FDA (Fourier Domain Adaptation\n",
    "\n",
    "\n",
    "def FDA (image, reference_images, beta_limit=0.1, read_fn='', always_apply=False, p=1.0):\n",
    "    \"\"\"Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n",
    "    Args:\n",
    "        clip_limit (float or (float, float)): upper threshold value for contrast limiting.\n",
    "            If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n",
    "        tile_grid_size ((int, int)): size of grid for histogram equalization. Default: (8, 8).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8\n",
    "    Refer:\n",
    "        https://github.com/YanchaoYang/FDA\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.FDAFDA (reference_images, beta_limit, read_fn, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### FancyPCA\n",
    "\n",
    "\n",
    "def FancyPCA (image,alpha=0.1, always_apply=False, p=1.0):\n",
    "    \"\"\"Augment RGB image using FancyPCA from Krizhevsky's paper\n",
    "    \"ImageNet Classification with Deep Convolutional Neural Networks\"\n",
    "    Args:\n",
    "        alpha (float):  how much to perturb/scale the eigen vecs and vals.\n",
    "            scale is samples from gaussian distribution (mu=0, sigma=alpha)\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        3-channel uint8 images only\n",
    "    Credit:\n",
    "        http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "        https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image\n",
    "        https://pixelatedbrian.github.io/2018-04-29-fancy_pca/\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.FancyPCA (alpha, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### FromFloat\n",
    "\n",
    "\n",
    "def FromFloat (image,dtype='uint16', max_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"Take an input array where all values should lie in the range [0, 1.0], multiply them by `max_value` and then\n",
    "    cast the resulted value to a type specified by `dtype`. If `max_value` is None the transform will try to infer\n",
    "    the maximum value for the data type from the `dtype` argument.\n",
    "    This is the inverse transform for :class:`~albumentations.augmentations.transforms.ToFloat`.\n",
    "    Args:\n",
    "        max_value (float): maximum possible input value. Default: None.\n",
    "        dtype (string or numpy data type): data type of the output. See the `'Data types' page from the NumPy docs`_.\n",
    "            Default: 'uint16'.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        float32\n",
    "    .. _'Data types' page from the NumPy docs:\n",
    "       https://docs.scipy.org/doc/numpy/user/basics.types.html\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.FromFloat (dtype, max_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### GaussNoise\n",
    "\n",
    "\n",
    "def GaussNoise (image,var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=1.0):\n",
    "    \"\"\"Apply gaussian noise to the input image.\n",
    "    Args:\n",
    "        var_limit ((float, float) or float): variance range for noise. If var_limit is a single float, the range\n",
    "            will be (0, var_limit). Default: (10.0, 50.0).\n",
    "        mean (float): mean of the noise. Default: 0\n",
    "        per_channel (bool): if set to True, noise will be sampled for each channel independently.\n",
    "            Otherwise, the noise will be sampled once for all channels. Default: True\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.GaussNoise (var_limit, mean, per_channel, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### GaussianBlur\n",
    "\n",
    "\n",
    "def GaussianBlur (image, blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=1.0):\n",
    "    \"\"\"Blur the input image using a Gaussian filter with a random kernel size.\n",
    "    Args:\n",
    "        blur_limit (int, (int, int)): maximum Gaussian kernel size for blurring the input image.\n",
    "            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n",
    "            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n",
    "            If set single value `blur_limit` will be in range (0, blur_limit).\n",
    "            Default: (3, 7).\n",
    "        sigma_limit (float, (float, float)): Gaussian kernel standard deviation. Must be greater in range [0, inf).\n",
    "            If set single value `sigma_limit` will be in range (0, sigma_limit).\n",
    "            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.GaussianBlur (blur_limit, sigma_limit, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### GlassBlur\n",
    "\n",
    "\n",
    "def GlassBlur (image,sigma=0.7, max_delta=4, iterations=2, always_apply=False, mode='fast', p=1.0):\n",
    "    \"\"\"Apply glass noise to the input image.\n",
    "    Args:\n",
    "        sigma (float): standard deviation for Gaussian kernel.\n",
    "        max_delta (int): max distance between pixels which are swapped.\n",
    "        iterations (int): number of repeats.\n",
    "            Should be in range [1, inf). Default: (2).\n",
    "        mode (str): mode of computation: fast or exact. Default: \"fast\".\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/1903.12261\n",
    "    |  https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.GlassBlur(sigma, max_delta, iterations, always_apply, mode, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### HistogramMatching\n",
    "\n",
    "\n",
    "def HistogramMatching (image,reference_images, blend_ratio=(0.5, 1.0), read_fn=\"\", always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Apply histogram matching. It manipulates the pixels of an input image so that its histogram matches\n",
    "    the histogram of the reference image. If the images have multiple channels, the matching is done independently\n",
    "    for each channel, as long as the number of channels is equal in the input image and the reference.\n",
    "    Histogram matching can be used as a lightweight normalisation for image processing,\n",
    "    such as feature matching, especially in circumstances where the images have been taken from different\n",
    "    sources or in different conditions (i.e. lighting).\n",
    "    See:\n",
    "        https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html\n",
    "    Args:\n",
    "        reference_images (List[str] or List(np.ndarray)): List of file paths for reference images\n",
    "            or list of reference images.\n",
    "        blend_ratio (float, float): Tuple of min and max blend ratio. Matched image will be blended with original\n",
    "            with random blend factor for increased diversity of generated images.\n",
    "        read_fn (Callable): Used-defined function to read image. Function should get image path and return numpy\n",
    "            array of image pixels.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, uint16, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.domain_adaptation.HistogramMatching (reference_images, blend_ratio, read_fn, always_apply, p) ])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### HueSaturationValue\n",
    "\n",
    "\n",
    "def HueSaturationValue (image,hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=1.0):\n",
    "    \"\"\"Randomly change hue, saturation and value of the input image.\n",
    "    Args:\n",
    "        hue_shift_limit ((int, int) or int): range for changing hue. If hue_shift_limit is a single int, the range\n",
    "            will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).\n",
    "        sat_shift_limit ((int, int) or int): range for changing saturation. If sat_shift_limit is a single int,\n",
    "            the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).\n",
    "        val_shift_limit ((int, int) or int): range for changing value. If val_shift_limit is a single int, the range\n",
    "            will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.HueSaturationValue (hue_shift_limit, sat_shift_limit, val_shift_limit, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ISONoise\n",
    "\n",
    "\n",
    "def ISONoise (image,color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Apply camera sensor noise.\n",
    "    Args:\n",
    "        color_shift (float, float): variance range for color hue change.\n",
    "            Measured as a fraction of 360 degree Hue angle in HLS colorspace.\n",
    "        intensity ((float, float): Multiplicative factor that control strength\n",
    "            of color and luminace noise.\n",
    "        p (float): probability of applying the transform. Default: 0.5.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ISONoise (color_shift, intensity, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ImageCompression\n",
    "\n",
    "\n",
    "def ImageCompression (image,quality_lower=99, quality_upper=100, compression_type=None, always_apply=False, p=1.0):\n",
    "    \"\"\"Decrease Jpeg, WebP compression of an image.\n",
    "    Args:\n",
    "        quality_lower (float): lower bound on the image quality.\n",
    "                               Should be in [0, 100] range for jpeg and [1, 100] for webp.\n",
    "        quality_upper (float): upper bound on the image quality.\n",
    "                               Should be in [0, 100] range for jpeg and [1, 100] for webp.\n",
    "        compression_type (ImageCompressionType): should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n",
    "            Default: ImageCompressionType.JPEG\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ImageCompression (quality_lower, quality_upper, compression_type, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### InvertImg\n",
    "\n",
    "\n",
    "def InvertImg(image,p=1.0):\n",
    "    \"\"\"Invert the input image by subtracting pixel values from 255.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.InvertImg(p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### MedianBlur\n",
    "\n",
    "\n",
    "def MedianBlur (image,blur_limit=7, always_apply=False, p=1.0):\n",
    "    \"\"\"Blur the input image using a median filter with a random aperture linear size.\n",
    "    Args:\n",
    "        blur_limit (int): maximum aperture linear size for blurring the input image.\n",
    "            Must be odd and in range [3, inf). Default: (3, 7).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.MedianBlur (blur_limit, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### MotionBlur\n",
    "\n",
    "\n",
    "def MotionBlur(image,blur_limit=7,p=1.0):\n",
    "    \"\"\"Apply motion blur to the input image using a random-sized kernel.\n",
    "    Args:\n",
    "        blur_limit (int): maximum kernel size for blurring the input image.\n",
    "            Should be in range [3, inf). Default: (3, 7).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.MotionBlur(blur_limit,p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### MultiplicativeNoise\n",
    "\n",
    "\n",
    "def MultiplicativeNoise (image,multiplier=(0.9, 1.1), per_channel=False, elementwise=False, always_apply=False, p=1.0):\n",
    "    \"\"\"Multiply image to random number or array of numbers.\n",
    "    Args:\n",
    "        multiplier (float or tuple of floats): If single float image will be multiplied to this number.\n",
    "            If tuple of float multiplier will be in range `[multiplier[0], multiplier[1])`. Default: (0.9, 1.1).\n",
    "        per_channel (bool): If `False`, same values for all channels will be used.\n",
    "            If `True` use sample values for each channels. Default False.\n",
    "        elementwise (bool): If `False` multiply multiply all pixels in an image with a random value sampled once.\n",
    "            If `True` Multiply image pixels with values that are pixelwise randomly sampled. Defaule: False.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        Any\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.MultiplicativeNoise (multiplier, per_channel, elementwise, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Normalize\n",
    "\n",
    "\n",
    "def Normalize (image,mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0):\n",
    "    \"\"\"Normalization is applied by the formula: `img = (img - mean * max_pixel_value) / (std * max_pixel_value)`\n",
    "    Args:\n",
    "        mean (float, list of float): mean values\n",
    "        std  (float, list of float): std values\n",
    "        max_pixel_value (float): maximum possible pixel value\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Normalize (mean, std, max_pixel_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### PixelDistributionAdaptation\n",
    "\n",
    "\n",
    "def PixelDistributionAdaptation (image,reference_images, blend_ratio=(0.25, 1.0), read_fn='', transform_type='pca', always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Another naive and quick pixel-level domain adaptation. It fits a simple transform (such as PCA, StandardScaler\n",
    "    or MinMaxScaler) on both original and reference image, transforms original image with transform trained on this\n",
    "    image and then performs inverse transformation using transform fitted on reference image.\n",
    "    Args:\n",
    "        reference_images (List[str] or List(np.ndarray)): List of file paths for reference images\n",
    "            or list of reference images.\n",
    "        blend_ratio (float, float): Tuple of min and max blend ratio. Matched image will be blended with original\n",
    "            with random blend factor for increased diversity of generated images.\n",
    "        read_fn (Callable): Used-defined function to read image. Function should get image path and return numpy\n",
    "            array of image pixels. Usually it's default `read_rgb_image` when images paths are used as reference,\n",
    "            otherwise it could be identity function `lambda x: x` if reference images have been read in advance.\n",
    "        transform_type (str): type of transform; \"pca\", \"standard\", \"minmax\" are allowed.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    See also: https://github.com/arsenyinfo/qudida\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.domain_adaptation.PixelDistributionAdaptation (reference_images, blend_ratio, read_fn, transform_type, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Posterize\n",
    "\n",
    "\n",
    "def Posterize (image,num_bits=4, always_apply=False, p=1.0):\n",
    "    \"\"\"Reduce the number of bits for each color channel.\n",
    "    Args:\n",
    "        num_bits ((int, int) or int,\n",
    "                  or list of ints [r, g, b],\n",
    "                  or list of ints [[r1, r1], [g1, g2], [b1, b2]]): number of high bits.\n",
    "            If num_bits is a single value, the range will be [num_bits, num_bits].\n",
    "            Must be in range [0, 8]. Default: 4.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "    image\n",
    "    Image types:\n",
    "        uint8\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Posterize (num_bits, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### RGBShift\n",
    "\n",
    "\n",
    "def RGBShift (image,r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, always_apply=False, p=1.0):\n",
    "    \"\"\"Randomly shift values for each channel of the input RGB image.\n",
    "    Args:\n",
    "        r_shift_limit ((int, int) or int): range for changing values for the red channel. If r_shift_limit is a single\n",
    "            int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20).\n",
    "        g_shift_limit ((int, int) or int): range for changing values for the green channel. If g_shift_limit is a\n",
    "            single int, the range  will be (-g_shift_limit, g_shift_limit). Default: (-20, 20).\n",
    "        b_shift_limit ((int, int) or int): range for changing values for the blue channel. If b_shift_limit is a single\n",
    "            int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.RGBShift (r_shift_limit, g_shift_limit, b_shift_limit, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Sharpen\n",
    "\n",
    "\n",
    "def Sharpen (image,alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=1.0):\n",
    "    \"\"\"Sharpen the input image and overlays the result with the original image.\n",
    "    Args:\n",
    "        alpha ((float, float)): range to choose the visibility of the sharpened image. At 0, only the original image is\n",
    "            visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).\n",
    "        lightness ((float, float)): range to choose the lightness of the sharpened image. Default: (0.5, 1.0).\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Sharpen (alpha, lightness, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Solarize\n",
    "\n",
    "\n",
    "def Solarize (image,threshold=128, always_apply=False, p=1.0):\n",
    "    \"\"\"Invert all pixel values above a threshold.\n",
    "    Args:\n",
    "        threshold ((int, int) or int, or (float, float) or float): range for solarizing threshold.\n",
    "        If threshold is a single value, the range will be [threshold, threshold]. Default: 128.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        any\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Solarize (threshold, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Superpixels\n",
    "\n",
    "\n",
    "def Superpixels (image, p_replace=0.1, n_segments=100, max_size=128, interpolation=1, always_apply=False, p=1.0):\n",
    "    \"\"\"Transform images partially/completely to their superpixel representation.\n",
    "    This implementation uses skimage's version of the SLIC algorithm.\n",
    "    Args:\n",
    "        p_replace (float or tuple of float): Defines for any segment the probability that the pixels within that\n",
    "            segment are replaced by their average color (otherwise, the pixels are not changed).\n",
    "            Examples:\n",
    "                * A probability of ``0.0`` would mean, that the pixels in no\n",
    "                  segment are replaced by their average color (image is not\n",
    "                  changed at all).\n",
    "                * A probability of ``1.0`` would mean, that around half of all\n",
    "                  segments are replaced by their average color.\n",
    "                * A probability of ``1.0`` would mean, that all segments are\n",
    "                  replaced by their average color (resulting in a voronoi\n",
    "                  image).\n",
    "            Behaviour based on chosen data types for this parameter:\n",
    "                * If a ``float``, then that ``flat`` will always be used.\n",
    "                * If ``tuple`` ``(a, b)``, then a random probability will be\n",
    "                  sampled from the interval ``[a, b]`` per image.\n",
    "        n_segments (int, or tuple of int): Rough target number of how many superpixels to generate (the algorithm\n",
    "            may deviate from this number). Lower value will lead to coarser superpixels.\n",
    "            Higher values are computationally more intensive and will hence lead to a slowdown\n",
    "            * If a single ``int``, then that value will always be used as the\n",
    "              number of segments.\n",
    "            * If a ``tuple`` ``(a, b)``, then a value from the discrete\n",
    "              interval ``[a..b]`` will be sampled per image.\n",
    "        max_size (int or None): Maximum image size at which the augmentation is performed.\n",
    "            If the width or height of an image exceeds this value, it will be\n",
    "            downscaled before the augmentation so that the longest side matches `max_size`.\n",
    "            This is done to speed up the process. The final output image has the same size as the input image.\n",
    "            Note that in case `p_replace` is below ``1.0``,\n",
    "            the down-/upscaling will affect the not-replaced pixels too.\n",
    "            Use ``None`` to apply no down-/upscaling.\n",
    "        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n",
    "            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n",
    "            Default: cv2.INTER_LINEAR.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Superpixels (p_replace, n_segments, max_size, interpolation, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ToFloat\n",
    "\n",
    "\n",
    "def ToFloat (image,max_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"Divide pixel values by `max_value` to get a float32 output array where all values lie in the range [0, 1.0].\n",
    "    If `max_value` is None the transform will try to infer the maximum value by inspecting the data type of the input\n",
    "    image.\n",
    "    See Also:\n",
    "        :class:`~albumentations.augmentations.transforms.FromFloat`\n",
    "    Args:\n",
    "        max_value (float): maximum possible input value. Default: None.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        any type\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ToFloat (max_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ToGray\n",
    "\n",
    "\n",
    "def ToGray(image,p=1.0):\n",
    "    \"\"\"Convert the input RGB image to grayscale. If the mean pixel value for the resulting image is greater\n",
    "    than 127, invert the resulting grayscale image.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ToGray(p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### ToSepia\n",
    "\n",
    "\n",
    "def ToSepia (image,always_apply=False, p=1.0):\n",
    "    \"\"\"Applies sepia filter to the input RGB image\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.ToSepia (always_apply=False, p=1.0)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### VerticalFlip\n",
    "\n",
    "\n",
    "def VerticalFlip (image, p=1.0):\n",
    "    \"\"\"Flip the input vertically around the x-axis.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bboxes, keypoints\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.VerticalFlip(p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### HorizontalFlip\n",
    "\n",
    "\n",
    "def HorizontalFlip (image, p=1.0):\n",
    "    \"\"\"Flip the input horizontally around the y-axis.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bboxes, keypoints\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.HorizontalFlip (p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Flip (Random_Flip)\n",
    "\n",
    "\n",
    "def Flip (image, p=1.0):\n",
    "    \"\"\"Flip the input either horizontally, vertically or both horizontally and vertically.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bboxes, keypoints\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Flip (p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "\n",
    "#d = random.randint(-1, 1)\n",
    "#https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L337\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def random_flip(img, code):\n",
    "    return cv2.flip(img, code)\n",
    "'''\n",
    "# https://github.com/albumentations-team/albumentations/blob/6de7dd01410a666c23c70cf69c548f171c94a1a7/albumentations/augmentations/functional.py#L119\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def bbox_vflip(bbox, rows, cols):  # skipcq: PYL-W0613\n",
    "    \"\"\"Flip a bounding box vertically around the x-axis.\n",
    "    Args:\n",
    "        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "        rows (int): Image rows.\n",
    "        cols (int): Image cols.\n",
    "    Returns:\n",
    "        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox[:4]\n",
    "    return x_min, 1 - y_max, x_max, 1 - y_min\n",
    "\n",
    "\n",
    "def bbox_hflip(bbox, rows, cols):  # skipcq: PYL-W0613\n",
    "    \"\"\"Flip a bounding box horizontally around the y-axis.\n",
    "    Args:\n",
    "        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "        rows (int): Image rows.\n",
    "        cols (int): Image cols.\n",
    "    Returns:\n",
    "        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox[:4]\n",
    "    return 1 - x_max, y_min, 1 - x_min, y_max\n",
    "\n",
    "def bbox_flip(bbox, d, rows, cols):\n",
    "    \"\"\"Flip a bounding box either vertically, horizontally or both depending on the value of `d`.\n",
    "    Args:\n",
    "        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "        d (int):\n",
    "        rows (int): Image rows.\n",
    "        cols (int): Image cols.\n",
    "    Returns:\n",
    "        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "    Raises:\n",
    "        ValueError: if value of `d` is not -1, 0 or 1.\n",
    "    \"\"\"\n",
    "    if d == 0:\n",
    "        bbox = bbox_vflip(bbox, rows, cols)\n",
    "    elif d == 1:\n",
    "        bbox = bbox_hflip(bbox, rows, cols)\n",
    "    elif d == -1:\n",
    "        bbox = bbox_hflip(bbox, rows, cols)\n",
    "        bbox = bbox_vflip(bbox, rows, cols)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid d value {}. Valid values are -1, 0 and 1\".format(d))\n",
    "    return bbox\n",
    "'''\n",
    "#https://github.com/albumentations-team/albumentations/blob/6de7dd01410a666c23c70cf69c548f171c94a1a7/albumentations/augmentations/functional.py#L1320\n",
    "\n",
    "\n",
    "# #### Transpose\n",
    "\n",
    "\n",
    "def Transpose (image, p=1.0):\n",
    "    \"\"\"Transpose the input by swapping rows and columns.\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bboxes, keypoints\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Transpose (p=1.0)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### OpticalDistortion\n",
    "\n",
    "\n",
    "def OpticalDistortion (image,distort_limit=0.05, shift_limit=0.05, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        distort_limit (float, (float, float)): If distort_limit is a single float, the range\n",
    "            will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).\n",
    "        shift_limit (float, (float, float))): If shift_limit is a single float, the range\n",
    "            will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).\n",
    "        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n",
    "            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n",
    "            Default: cv2.INTER_LINEAR.\n",
    "        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n",
    "            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n",
    "            Default: cv2.BORDER_REFLECT_101\n",
    "        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n",
    "        mask_value (int, float,\n",
    "                    list of ints,\n",
    "                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n",
    "    Targets:\n",
    "        image, mask\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.OpticalDistortion (distort_limit, shift_limit, interpolation, border_mode, value, mask_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### GridDistortion\n",
    "\n",
    "\n",
    "def GridDistortion (image,num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=1) :\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_steps (int): count of grid cells on each side.\n",
    "        distort_limit (float, (float, float)): If distort_limit is a single float, the range\n",
    "            will be (-distort_limit, distort_limit). Default: (-0.03, 0.03).\n",
    "        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n",
    "            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n",
    "            Default: cv2.INTER_LINEAR.\n",
    "        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n",
    "            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n",
    "            Default: cv2.BORDER_REFLECT_101\n",
    "        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n",
    "        mask_value (int, float,\n",
    "                    list of ints,\n",
    "                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n",
    "    Targets:\n",
    "        image, mask\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.GridDistortion (num_steps, distort_limit, interpolation, border_mode, value, mask_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### PadIfNeeded\n",
    "\n",
    "\n",
    "def PadIfNeeded (image,min_height=1024, min_width=1024, pad_height_divisor=None, pad_width_divisor=None, position=None, border_mode=4, value=None, mask_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"Pad side of the image / max if side is less than desired number.\n",
    "    Args:\n",
    "        min_height (int): minimal result image height.\n",
    "        min_width (int): minimal result image width.\n",
    "        pad_height_divisor (int): if not None, ensures image height is dividable by value of this argument.\n",
    "        pad_width_divisor (int): if not None, ensures image width is dividable by value of this argument.\n",
    "        position (Union[str, PositionType]): Position of the image. should be PositionType.CENTER or\n",
    "            PositionType.TOP_LEFT or PositionType.TOP_RIGHT or PositionType.BOTTOM_LEFT or PositionType.BOTTOM_RIGHT.\n",
    "            Default: PositionType.CENTER.\n",
    "        border_mode (OpenCV flag): OpenCV border mode.\n",
    "        value (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n",
    "        mask_value (int, float,\n",
    "                    list of int,\n",
    "                    list of float): padding value for mask if border_mode is cv2.BORDER_CONSTANT.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bbox, keypoints\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.PadIfNeeded (min_height, min_width, pad_height_divisor, pad_width_divisor, position, border_mode, value, mask_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### JpegCompression\n",
    "\n",
    "\n",
    "def JpegCompression (image,quality_lower=99, quality_upper=100, always_apply=False, p=1.0):\n",
    "    \"\"\"Decrease Jpeg compression of an image.\n",
    "    Args:\n",
    "        quality_lower (float): lower bound on the jpeg quality. Should be in [0, 100] range\n",
    "        quality_upper (float): upper bound on the jpeg quality. Should be in [0, 100] range\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.JpegCompression (quality_lower, quality_upper, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Cutout\n",
    "\n",
    "\n",
    "def Cutout (image,num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=1.0):\n",
    "    \"\"\"CoarseDropout of the square regions in the image.\n",
    "    Args:\n",
    "        num_holes (int): number of regions to zero out\n",
    "        max_h_size (int): maximum height of the hole\n",
    "        max_w_size (int): maximum width of the hole\n",
    "        fill_value (int, float, list of int, list of float): value for dropped pixels.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/1708.04552\n",
    "    |  https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n",
    "    |  https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Cutout (num_holes, max_h_size, max_w_size, fill_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### CoarseDropout\n",
    "\n",
    "\n",
    "def CoarseDropout (image,max_holes=8, max_height=8, max_width=8, min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"CoarseDropout of the rectangular regions in the image.\n",
    "    Args:\n",
    "        max_holes (int): Maximum number of regions to zero out.\n",
    "        max_height (int, float): Maximum height of the hole.\n",
    "        If float, it is calculated as a fraction of the image height.\n",
    "        max_width (int, float): Maximum width of the hole.\n",
    "        If float, it is calculated as a fraction of the image width.\n",
    "        min_holes (int): Minimum number of regions to zero out. If `None`,\n",
    "            `min_holes` is be set to `max_holes`. Default: `None`.\n",
    "        min_height (int, float): Minimum height of the hole. Default: None. If `None`,\n",
    "            `min_height` is set to `max_height`. Default: `None`.\n",
    "            If float, it is calculated as a fraction of the image height.\n",
    "        min_width (int, float): Minimum width of the hole. If `None`, `min_height` is\n",
    "            set to `max_width`. Default: `None`.\n",
    "            If float, it is calculated as a fraction of the image width.\n",
    "        fill_value (int, float, list of int, list of float): value for dropped pixels.\n",
    "        mask_fill_value (int, float, list of int, list of float): fill value for dropped pixels\n",
    "            in mask. If `None` - mask is not affected. Default: `None`.\n",
    "    Targets:\n",
    "        image, mask\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/1708.04552\n",
    "    |  https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n",
    "    |  https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.CoarseDropout (max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value, mask_fill_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### Lambda\n",
    "\n",
    "\n",
    "def Lambda (image=None, mask=None, keypoint=None, bbox=None, name=None, always_apply=False, p=1.0):\n",
    "    \"\"\"A flexible transformation class for using user-defined transformation functions per targets.\n",
    "    Function signature must include **kwargs to accept optinal arguments like interpolation method, image size, etc:\n",
    "    Args:\n",
    "        image (callable): Image transformation function.\n",
    "        mask (callable): Mask transformation function.\n",
    "        keypoint (callable): Keypoint transformation function.\n",
    "        bbox (callable): BBox transformation function.\n",
    "        always_apply (bool): Indicates whether this transformation should be always applied.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image, mask, bboxes, keypoints\n",
    "    Image types:\n",
    "        Any\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.Lambda (image, mask, keypoint, bbox, name, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### MaskDropout\n",
    "\n",
    "\n",
    "def MaskDropout (image, max_objects=1, image_fill_value=0, mask_fill_value=0, always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Image & mask augmentation that zero out mask and image regions corresponding\n",
    "    to randomly chosen object instance from mask.\n",
    "    Mask must be single-channel image, zero values treated as background.\n",
    "    Image can be any number of channels.\n",
    "    Inspired by https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.MaskDropout (max_objects, image_fill_value, mask_fill_value, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### GridDropout\n",
    "\n",
    "\n",
    "def GridDropout (image,ratio=0.5, unit_size_min=None, unit_size_max=None, holes_number_x=None, holes_number_y=None, shift_x=0, shift_y=0, random_offset=False, fill_value=0, mask_fill_value=None, always_apply=False, p=1.0):\n",
    "    \"\"\"GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.\n",
    "    Args:\n",
    "        ratio (float): the ratio of the mask holes to the unit_size (same for horizontal and vertical directions).\n",
    "            Must be between 0 and 1. Default: 0.5.\n",
    "        unit_size_min (int): minimum size of the grid unit. Must be between 2 and the image shorter edge.\n",
    "            If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: `None`.\n",
    "        unit_size_max (int): maximum size of the grid unit. Must be between 2 and the image shorter edge.\n",
    "            If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: `None`.\n",
    "        holes_number_x (int): the number of grid units in x direction. Must be between 1 and image width//2.\n",
    "            If 'None', grid unit width is set as image_width//10. Default: `None`.\n",
    "        holes_number_y (int): the number of grid units in y direction. Must be between 1 and image height//2.\n",
    "            If `None`, grid unit height is set equal to the grid unit width or image height, whatever is smaller.\n",
    "        shift_x (int): offsets of the grid start in x direction from (0,0) coordinate.\n",
    "            Clipped between 0 and grid unit_width - hole_width. Default: 0.\n",
    "        shift_y (int): offsets of the grid start in y direction from (0,0) coordinate.\n",
    "            Clipped between 0 and grid unit height - hole_height. Default: 0.\n",
    "        random_offset (boolean): weather to offset the grid randomly between 0 and grid unit size - hole size\n",
    "            If 'True', entered shift_x, shift_y are ignored and set randomly. Default: `False`.\n",
    "        fill_value (int): value for the dropped pixels. Default = 0\n",
    "        mask_fill_value (int): value for the dropped pixels in mask.\n",
    "            If `None`, transformation is not applied to the mask. Default: `None`.\n",
    "    Targets:\n",
    "        image, mask\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    References:\n",
    "        https://arxiv.org/abs/2001.04086\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.GridDropout (ratio, unit_size_min, unit_size_max, holes_number_x, holes_number_y, shift_x, shift_y, random_offset, fill_value, mask_fill_value, always_apply, p) ])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### TemplateTransform\n",
    "\n",
    "\n",
    "def TemplateTransform (image,templates=None, img_weight=0.5, template_weight=0.5, template_transform=None, name=None, always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Apply blending of input image with specified templates\n",
    "    Args:\n",
    "        templates (numpy array or list of numpy arrays): Images as template for transform.\n",
    "        img_weight ((float, float) or float): If single float will be used as weight for input image.\n",
    "            If tuple of float img_weight will be in range `[img_weight[0], img_weight[1])`. Default: 0.5.\n",
    "        template_weight ((float, float) or float): If single float will be used as weight for template.\n",
    "            If tuple of float template_weight will be in range `[template_weight[0], template_weight[1])`.\n",
    "            Default: 0.5.\n",
    "        template_transform: transformation object which could be applied to template,\n",
    "            must produce template the same size as input image.\n",
    "        name (string): (Optional) Name of transform, used only for deserialization.\n",
    "        p (float): probability of applying the transform. Default: 1.0.\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.TemplateTransform (templates, img_weight, template_weight, template_transform, name, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### RingingOvershoot\n",
    "\n",
    "\n",
    "def RingingOvershoot (image,blur_limit=(7, 15), cutoff=(0.7853981633974483, 1.5707963267948966), always_apply=False, p=1.0):\n",
    "    \"\"\"Create ringing or overshoot artefacts by conlvolving image with 2D sinc filter.\n",
    "    Args:\n",
    "        blur_limit (int, (int, int)): maximum kernel size for sinc filter.\n",
    "            Should be in range [3, inf). Default: (7, 15).\n",
    "        cutoff (float, (float, float)): range to choose the cutoff frequency in radians.\n",
    "            Should be in range (0, np.pi)\n",
    "            Default: (np.pi / 4, np.pi / 2).\n",
    "        p (float): probability of applying the transform. Default: 1.0\n",
    "    Reference:\n",
    "        dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n",
    "        https://arxiv.org/abs/2107.10833\n",
    "    Targets:\n",
    "        image\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.RingingOvershoot (blur_limit, cutoff, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# #### UnsharpMask\n",
    "\n",
    "\n",
    "def UnsharpMask (image,blur_limit=(3, 7), sigma_limit=0.0, alpha=(0.2, 1.0), threshold=10, always_apply=False, p=1.0):\n",
    "    \"\"\"\n",
    "    Sharpen the input image using Unsharp Masking processing and overlays the result with the original image.\n",
    "    Args:\n",
    "        blur_limit (int, (int, int)): maximum Gaussian kernel size for blurring the input image.\n",
    "            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n",
    "            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n",
    "            If set single value `blur_limit` will be in range (0, blur_limit).\n",
    "            Default: (3, 7).\n",
    "        sigma_limit (float, (float, float)): Gaussian kernel standard deviation. Must be in range [0, inf).\n",
    "            If set single value `sigma_limit` will be in range (0, sigma_limit).\n",
    "            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n",
    "        alpha (float, (float, float)): range to choose the visibility of the sharpened image.\n",
    "            At 0, only the original image is visible, at 1.0 only its sharpened version is visible.\n",
    "            Default: (0.2, 0.5).\n",
    "        threshold (int): Value to limit sharpening only for areas with high pixel difference between original image\n",
    "            and it's smoothed version. Higher threshold means less sharpening on flat areas.\n",
    "            Must be in range [0, 255]. Default: 10.\n",
    "        p (float): probability of applying the transform. Default: 1.0\n",
    "    Reference:\n",
    "        arxiv.org/pdf/2107.10833.pdf\n",
    "    Targets:\n",
    "        image\n",
    "    \"\"\"\n",
    "        \n",
    "    transform= transform = A.Compose([A.augmentations.transforms.UnsharpMask (blur_limit, sigma_limit, alpha, threshold, always_apply, p)])\n",
    "    transformed = transform(image=image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "# ## Read & Display Images\n",
    "\n",
    "# #### Read image Custom funtion\n",
    "\n",
    "\n",
    "def readImage(image_path):\n",
    "    \n",
    "    imageInBGR= cv2.imread(image_path)\n",
    "    imageBGR2RGB=cv2.cvtColor(imageInBGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return imageBGR2RGB\n",
    "\n",
    "\n",
    "# #### Display Image\n",
    "\n",
    "\n",
    "def visualize(image):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"OFF\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ### Custom Data Generator\n",
    "\n",
    "\n",
    "class StaticTransformDataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, labels, image_path, \n",
    "                 to_fit=True, num_sample=1, dim=(256, 256),\n",
    "                 n_channels=3, n_classes=10, shuffle=True):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param num_sample: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.image_path = image_path\n",
    "        #self.mask_path = mask_path\n",
    "        self.to_fit = to_fit\n",
    "        self.num_sample = num_sample\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "# First, we define the constructor to initialize the configuration of the generator. \n",
    "# we assume the path to the data is in a dataframe column. \n",
    "# Hence, we define the x_col and y_col parameters. \n",
    "# This could also be a directory name from where you can load the data.\n",
    "\n",
    "    #Another utility method we have is __len__. \n",
    "    #It essentially returns the number of steps in an epoch, using the samples and the batch size.\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.num_sample:(index + 1) * self.num_sample]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_X(list_IDs_temp)\n",
    "\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_X(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing num_sample images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.num_sample, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self._load_grayscale_image(self.image_path + self.labels[ID])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing num_sample masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        y = np.empty((self.num_sample, *self.dim), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            y[i,] = self._load_grayscale_image(self.mask_path + self.labels[ID])\n",
    "\n",
    "        return y\n",
    "\n",
    "    def Blur (self,image,num_sample,blur_limit=7, always_apply=False, p=1.0):\n",
    "        for i in range(num_sample):\n",
    "            img=Blur(image,blur_limit, always_apply, p)\n",
    "            transform_type='Blur'\n",
    "            return img  \n",
    "\n",
    "    def CLAHE (self,image,num_sample,clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=1.0):\n",
    "        for i in range(num_sample):\n",
    "            img=CLAHE(image,clip_limit, tile_grid_size, always_apply, p)\n",
    "            transform_type='CLAHE'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def ChannelDropout(self,image,num_sample,channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1.0):\n",
    "        for i in range(num_sample):\n",
    "            img=ChannelDropout(image)\n",
    "            transform_type='ChannelDropout'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def ChannelShuffle (self,image,num_sample,p=1.0):\n",
    "        for i in range(num_sample):\n",
    "            img=ChannelShuffle(image,p)\n",
    "            transform_type='ChannelShuffle'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def ColorJitter (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ColorJitter(image)\n",
    "            transform_type='ColorJitter'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def Downscale (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Downscale(image)\n",
    "            transform_type='Downscale'\n",
    "            return img  \n",
    "         \n",
    "\n",
    "    def Emboss (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Emboss(image)\n",
    "            transform_type='Emboss'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def Equalize (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Equalize(image)\n",
    "            transform_type='Equalize'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def FDA (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=FDA(image)\n",
    "            transform_type='FDA'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def FancyPCA (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=FancyPCA(image)\n",
    "            transform_type='FancyPCA'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def FromFloat (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=FromFloat(image)\n",
    "            transform_type='FromFloat'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def GaussNoise (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=GaussNoise(image)\n",
    "            transform_type='GaussNoise'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def GaussianBlur (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=GaussianBlur(image)\n",
    "            transform_type='GaussianBlur'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def GlassBlur (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=GlassBlur(image)\n",
    "            transform_type='GlassBlur'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def HistogramMatching (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=HistogramMatching(image)\n",
    "            transform_type='HistogramMatching'\n",
    "            return img           \n",
    "        \n",
    "\n",
    "    def HueSaturationValue (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=HueSaturationValue(image)\n",
    "            transform_type='HueSaturationValue'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def ISONoise (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ISONoise(image)\n",
    "            transform_type='ISONoise'\n",
    "            return img  \n",
    "        \n",
    "\n",
    "    def ImageCompression (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ImageCompression(image)\n",
    "            transform_type='ImageCompression'\n",
    "            return img  \n",
    "        \n",
    "    def InvertImg (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=InvertImg(image)\n",
    "            transform_type='InvertImg'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def MedianBlur (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=MedianBlur(image)\n",
    "            transform_type='MedianBlur'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def MotionBlur (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=MotionBlur(image)\n",
    "            transform_type='MotionBlur'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def MultiplicativeNoise (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=MultiplicativeNoise(image)\n",
    "            transform_type='MultiplicativeNoise'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "        \n",
    "    def Normalize (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Normalize(image)\n",
    "            transform_type='Normalize'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def PixelDistributionAdaptation (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=PixelDistributionAdaptation(image)\n",
    "            transform_type='PixelDistributionAdaptation'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Posterize (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Posterize(image)\n",
    "            transform_type='Posterize'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def RGBShift (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=RGBShift(image)\n",
    "            transform_type='RGBShift'\n",
    "            return img  \n",
    "        \n",
    "    def Sharpen (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Sharpen(image)\n",
    "            transform_type='Sharpen'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Solarize (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Solarize(image)\n",
    "            transform_type='Solarize'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Superpixels (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Superpixels(image)\n",
    "            transform_type='Superpixels'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def ToFloat (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ToFloat(image)\n",
    "            transform_type='ToFloat'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def ToGray (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ToGray(image)\n",
    "            transform_type='ToGray'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def ToSepia (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=ToSepia(image)\n",
    "            transform_type='ToSepia'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def VerticalFlip (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=VerticalFlip(image)\n",
    "            transform_type='VerticalFlip'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def HorizontalFlip (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=HorizontalFlip(image)\n",
    "            transform_type='HorizontalFlip'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Flip (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Flip(image)\n",
    "            transform_type='Flip'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Transpose (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Transpose(image)\n",
    "            transform_type='Transpose'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def OpticalDistortion (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=OpticalDistortion(image)\n",
    "            transform_type='OpticalDistortion'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def GridDistortion (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=GridDistortion(image)\n",
    "            transform_type='GridDistortion'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def PadIfNeeded (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=PadIfNeeded(image)\n",
    "            transform_type='PadIfNeeded'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def JpegCompression (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=JpegCompression(image)\n",
    "            transform_type='JpegCompression'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Cutout (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Cutout(image)\n",
    "            transform_type='Cutout'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def CoarseDropout (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=CoarseDropout(image)\n",
    "            transform_type='CoarseDropout'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def Lambda (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=Lambda(image)\n",
    "            transform_type='Lambda'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def MaskDropout (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=MaskDropout(image)\n",
    "            transform_type='MaskDropout'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def GridDropout (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=GridDropout(image)\n",
    "            transform_type='GridDropout'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def TemplateTransform (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=TemplateTransform(image)\n",
    "            transform_type='TemplateTransform'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def RingingOvershoot (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=RingingOvershoot(image)\n",
    "            transform_type='RingingOvershoot'\n",
    "            return img  \n",
    "    \n",
    "        \n",
    "    def UnsharpMask (self,image,num_sample):\n",
    "        for i in range(num_sample):\n",
    "            img=UnsharpMask(image)\n",
    "            transform_type='UnsharpMask'\n",
    "            return img  \n",
    "    \n",
    "\n",
    "    def _load_grayscale_image(self, image_path):\n",
    "        \"\"\"Load grayscale image\n",
    "        :param image_path: path to image to load\n",
    "        :return: loaded image\n",
    "        \"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img / 255\n",
    "        return img\n",
    "    \n",
    "\n",
    "\n",
    "image_path=\"./Input_Images/*.jpg\"\n",
    "Transformed_images_path=\"./Transformed_Images\"\n",
    "#an_object= DataGenerator([1],\"dog\",image_path) \n",
    "#image = readImage(image_path)\n",
    "#print(an_object)\n",
    "filenames = glob.glob(image_path)\n",
    "input_image_id=0\n",
    "label='dog'\n",
    "for filename in filenames:\n",
    "    file_Name=filename.split('/')[-1]\n",
    "    an_object= DataGenerator([input_image_id],label,filename)\n",
    "    print(an_object)\n",
    "    image = readImage(filename)\n",
    "    transformed_image=an_object.Flip (image)\n",
    "    os,chdir(Transformed_images_path)\n",
    "    destination_filename=\"transformed\"+file_Name\n",
    "    cv2.imwrite(destination_filename, transformed_image) \n",
    "\n",
    "    print(transformed_image)\n",
    "    \n",
    "    \n",
    "    input_image_id+=1\n",
    "#visualize(image)\n",
    "#visualize(an_object.composing_random_transforms(image))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocessing_of_images(image_path):\n",
    "    image= readImage(image_path)\n",
    "    transformed_image=composing_random_transforms(image)\n",
    "    visualize (transformed_image)\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "destination_dir_for_transformed_images='./transformed_images/*.jpeg'\n",
    "transformed_images_filenames = glob.glob(destination_dir_for_transformed_images)\n",
    "\n",
    "for image in transformed_images_filenames:\n",
    "    transformed_img= readImage(image)\n",
    "    visualize(transformed_img)\n",
    "\n",
    "for image in transformed_images_filenames:\n",
    "    transformed_img= readImage(image)\n",
    "    \n",
    "    transformed_PIL_image = Image.fromarray(np.uint8(transformed_img)).convert('RGB')    \n",
    "    #input_tensor=preprocess(torch.Tensor(transformed_img))\n",
    "    input_tensor=preprocess(transformed_PIL_image)\n",
    "    print(input_tensor)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    print(input_batch)\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "    print('cuda available')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "print(\"random_crop\")\n",
    "random_crop(im2arr)\n",
    "#random_resize(im2arr,1,2)\n",
    "random_scale(im2arr)\n",
    "random_rotate(im2arr)\n",
    " random_shift_scale_rotate(im2arr)'''\n",
    "\n",
    "# %% [markdown]\n",
    "# # ================================================================\n",
    "# %% [markdown]\n",
    "# ## MODEL for prediction of scores\n",
    "# %% [markdown]\n",
    "# ### Load a Resnet Model\n",
    "\n",
    "# %%\n",
    "'''import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])'''\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "'''\n",
    "for image in transformed_images_filenames:\n",
    "    transformed_img= readImage(image)\n",
    "    \n",
    "    transformed_PIL_image = Image.fromarray(np.uint8(transformed_img)).convert('RGB')    \n",
    "    #input_tensor=preprocess(torch.Tensor(transformed_img))\n",
    "    input_tensor=preprocess(transformed_PIL_image)\n",
    "    print(input_tensor)\n",
    "'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''destination_dir_for_transformed_images='./transformed_images/*.jpeg'\n",
    "transformed_images_filenames = glob.glob(destination_dir_for_transformed_images)\n",
    "print(type(transformed_images_filenames))'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "print(input_batch)'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "    print('cuda available')'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''with torch.no_grad():\n",
    "    output = model(input_batch)'''\n",
    "\n",
    "# %% [markdown]\n",
    "# ### The output has unnormalized scores. To get probabilities, we are running a softmax on it.\n",
    "# \n",
    "\n",
    "# %%\n",
    "'''probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# probabilities's type <class 'torch.Tensor'>\n",
    "probabilities_nparray=probabilities.cpu().detach().numpy()\n",
    "#np.argmax(probabilities)\n",
    "#print(np.argmax(probabilities_nparray))\n",
    "index=np.argmax(probabilities_nparray)\n",
    "score=probabilities_nparray[index]\n",
    "print(index,score)\n",
    "# print(type(probabilities_nparray)) = <class 'numpy.ndarray'>\n",
    "'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''def probabilities_get(input_batch):\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "        #print('cuda available')\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    probabilities_nparray=probabilities.cpu().detach().numpy()\n",
    "    #np.argmax(probabilities)\n",
    "    #print(np.argmax(probabilities_nparray))\n",
    "    index=np.argmax(probabilities_nparray)\n",
    "    score=probabilities_nparray[index]\n",
    "    return index,score\n",
    "'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''def max_probabilities_dict_func(predicted_id,image,class_index,prediction_score):\n",
    "    max_probabilities_dict=dict()\n",
    "    max_probabilities_dict['id'] = predicted_id\n",
    "    max_probabilities_dict['Image_Name']   = image.split('/')[-1]\n",
    "    max_probabilities_dict['class_index']   = class_index\n",
    "    max_probabilities_dict['prediction_score']   = prediction_score  \n",
    "\n",
    "    return max_probabilities_dict\n",
    "'''\n",
    "\n",
    "\n",
    "# %%\n",
    "'''\n",
    "probabilities_dict={}\n",
    "max_probabilities_list=[]\n",
    "predicted_id=0\n",
    "for image in transformed_images_filenames:\n",
    "    probabilities_scores_list=[]\n",
    "    class_index_list=[]\n",
    "    transformed_img= readImage(image)\n",
    "    \n",
    "    transformed_PIL_image = Image.fromarray(np.uint8(transformed_img)).convert('RGB')    \n",
    "    #input_tensor=preprocess(torch.Tensor(transformed_img))\n",
    "    input_tensor=preprocess(transformed_PIL_image)\n",
    "    #print(input_tensor)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    #print(input_batch)\n",
    "    class_index,prediction_score=probabilities_get(input_batch)\n",
    "    print(predicted_id,class_index,prediction_score)\n",
    "    \n",
    "    #class_index_list=class_index_list.append(class_index)\n",
    "    #probabilities_scores_list=probabilities_scores_list.append(prediction_score)\n",
    "\n",
    "    predicted_id+=1\n",
    "    \n",
    "    if prediction_score>0.7:\n",
    "        \n",
    "        probabilities_dict=max_probabilities_dict_func(predicted_id,image,class_index,prediction_score)\n",
    "        max_probabilities_list.append(probabilities_dict)\n",
    "print(max_probabilities_list)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "for image in transformed_images_filenames:\n",
    "    transformed_img= readImage(image)\n",
    "    \n",
    "    transformed_PIL_image = Image.fromarray(np.uint8(transformed_img)).convert('RGB')    \n",
    "    #input_tensor=preprocess(torch.Tensor(transformed_img))\n",
    "    input_tensor=preprocess(transformed_PIL_image)\n",
    "    print(input_tensor)\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc27d1cf61a468af7978b07bd4ca8341b99ee741c81921f0ccfd9bc7caf47c69"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('albumenation': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
